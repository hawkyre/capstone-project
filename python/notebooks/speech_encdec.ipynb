{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60be72de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\hawky\\anaconda3\\lib\\site-packages (21.2.4)\n",
      "Requirement already satisfied: install in c:\\users\\hawky\\anaconda3\\lib\\site-packages (1.3.5)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\hawky\\anaconda3\\lib\\site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hawky\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.11.0\n",
      "Collecting soundfile\n",
      "  Using cached soundfile-0.11.0-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\hawky\\anaconda3\\lib\\site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hawky\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f173af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b949bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio_file(filename):\n",
    "    sig, sr = torchaudio.load(filename)\n",
    "    return (sig, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b0b5d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0254, -0.0061,  0.0029]]), 16000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer, rate = read_audio_file('audiotest1.wav')\n",
    "buffer, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75e4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c914f90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab949bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "choose a window size 400 that is [2, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m processor \u001b[38;5;241m=\u001b[39m Speech2TextProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/s2t-small-librispeech-asr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[0;32m      8\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\speech_to_text\\processing_speech_to_text.py:73\u001b[0m, in \u001b[0;36mSpeech2TextProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(audio, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\speech_to_text\\feature_extraction_speech_to_text.py:220\u001b[0m, in \u001b[0;36mSpeech2TextFeatureExtractor.__call__\u001b[1;34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_tensors, sampling_rate, return_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [raw_speech]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# extract fbank features\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_fbank_features(waveform) \u001b[38;5;28;01mfor\u001b[39;00m waveform \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# convert into correct format for padding\u001b[39;00m\n\u001b[0;32m    223\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m BatchFeature({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: features})\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\speech_to_text\\feature_extraction_speech_to_text.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    217\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [raw_speech]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# extract fbank features\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_fbank_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m waveform \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# convert into correct format for padding\u001b[39;00m\n\u001b[0;32m    223\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m BatchFeature({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: features})\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\speech_to_text\\feature_extraction_speech_to_text.py:90\u001b[0m, in \u001b[0;36mSpeech2TextFeatureExtractor._extract_fbank_features\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m     88\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m15\u001b[39m)  \u001b[38;5;66;03m# Kaldi compliance: 16-bit signed integers\u001b[39;00m\n\u001b[0;32m     89\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(waveform)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mta_kaldi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfbank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_mel_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_mel_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchaudio\\compliance\\kaldi.py:591\u001b[0m, in \u001b[0;36mfbank\u001b[1;34m(waveform, blackman_coeff, channel, dither, energy_floor, frame_length, frame_shift, high_freq, htk_compat, low_freq, min_duration, num_mel_bins, preemphasis_coefficient, raw_energy, remove_dc_offset, round_to_power_of_two, sample_frequency, snip_edges, subtract_mean, use_energy, use_log_fbank, use_power, vtln_high, vtln_low, vtln_warp, window_type)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Create a fbank from a raw audio signal. This matches the input/output of Kaldi's\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03mcompute-fbank-feats.\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;124;03m    where m is calculated in _get_strided\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    589\u001b[0m device, dtype \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mdevice, waveform\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m--> 591\u001b[0m waveform, window_shift, window_size, padded_window_size \u001b[38;5;241m=\u001b[39m \u001b[43m_get_waveform_and_window_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_shift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_to_power_of_two\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreemphasis_coefficient\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(waveform) \u001b[38;5;241m<\u001b[39m min_duration \u001b[38;5;241m*\u001b[39m sample_frequency:\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# signal is too short\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchaudio\\compliance\\kaldi.py:142\u001b[0m, in \u001b[0;36m_get_waveform_and_window_properties\u001b[1;34m(waveform, channel, sample_frequency, frame_shift, frame_length, round_to_power_of_two, preemphasis_coefficient)\u001b[0m\n\u001b[0;32m    139\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sample_frequency \u001b[38;5;241m*\u001b[39m frame_length \u001b[38;5;241m*\u001b[39m MILLISECONDS_TO_SECONDS)\n\u001b[0;32m    140\u001b[0m padded_window_size \u001b[38;5;241m=\u001b[39m _next_power_of_2(window_size) \u001b[38;5;28;01mif\u001b[39;00m round_to_power_of_two \u001b[38;5;28;01melse\u001b[39;00m window_size\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m window_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(waveform), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose a window size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m that is [2, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    143\u001b[0m     window_size, \u001b[38;5;28mlen\u001b[39m(waveform)\n\u001b[0;32m    144\u001b[0m )\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m window_shift, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`window_shift` must be greater than 0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m padded_window_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe padded `window_size` must be divisible by two.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m use `round_to_power_of_two` or change `frame_length`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: choose a window size 400 that is [2, 1]"
     ]
    }
   ],
   "source": [
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "# ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = processor(buffer, sampling_rate=16000, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdf4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ef747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa365d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
