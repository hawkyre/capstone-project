{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(A):\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import fasttext\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "# pipe = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textmodel = fasttext.load_model('fasttext_model.bin')\n",
    "\n",
    "def predict_lang(sentence):\n",
    "    return textmodel.predict([sentence])[0][0][-1][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv():\n",
    "    df = pd.read_csv(\"data/model-counsel-chat.csv\", encoding='UTF8')\n",
    "    df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "    answers_in_english = []\n",
    "    for a in df['answer']:\n",
    "        label = pipe(a[:min(len(a)+1, 512)])[0]['label']\n",
    "        answers_in_english.append(label == 'en')\n",
    "        if label != 'en':\n",
    "            print(\"Text not in english: {}\".format(a))\n",
    "        print(\"Processed {} texts\".format(len(answers_in_english)))\n",
    "\n",
    "    print(answers_in_english)\n",
    "    df = df[answers_in_english]\n",
    "    print(df)\n",
    "    df.to_csv('data/model-counsel-chat-eng.csv', encoding='UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/model-counsel-chat.csv\", encoding='latin1')\n",
    "df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "\n",
    "dot_caps_regex = re.compile(r'(?<=\\.)(?=[A-Z])')\n",
    "newline_regex = re.compile(r'\\n|(\\r\\n)')\n",
    "weird_regex = re.compile(r'Â\\xa0')\n",
    "\n",
    "file_docs = []\n",
    "for i in range(len(df['answer'])):\n",
    "    row = df.iloc[i]\n",
    "    topic = row[0]\n",
    "    sentence = row[1]\n",
    "    print(topic, sentence)\n",
    "    tokens = sent_tokenize(sentence)\n",
    "    tokens = [weird_regex.sub(\" \", tok) for tok in tokens]\n",
    "    tokens = [dot_caps_regex.split(newline_regex.sub(\" \", tok)) for tok in tokens]\n",
    "    tokens = flatten(tokens)\n",
    "\n",
    "    for line in tokens:\n",
    "        # print(line)\n",
    "        lang = predict_lang(line)\n",
    "        if lang == 'en':\n",
    "            file_docs.append((line, topic))\n",
    "        else:\n",
    "            print(\"Sentence not in english (in {}): {}\".format(lang, line))\n",
    "\n",
    "print(\"Number of documents:\", len(file_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_file_docs = {}\n",
    "\n",
    "for line, topic in file_docs:\n",
    "    if not grouped_file_docs.get(topic):\n",
    "        grouped_file_docs.setdefault(topic, [line])\n",
    "    else:\n",
    "        grouped_file_docs[topic].append(line)\n",
    "\n",
    "for topic in grouped_file_docs:\n",
    "    docs = grouped_file_docs[topic]\n",
    "    grouped_file_docs[topic] = ' '.join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs = grouped_file_docs\n",
    "topics = list(file_docs.keys())\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topìcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtz = WordNetLemmatizer()\n",
    "stmr = PorterStemmer()\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = [w.lower() for w in word_tokenize(sentence)]\n",
    "    sentence = [word for word in sentence if word not in eng_stopwords]\n",
    "    sentence = [stmr.stem(lmtz.lemmatize(word)) for word in sentence]\n",
    "    sentence = [word for word in sentence if re.match(r\"^[a-z]+$\", word)]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [preprocess_sentence(file_docs[topic]) for topic in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)\n",
    "\n",
    "dictionary.save('text_to_context.dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print(doc)\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.save('text_to_context.tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('tfidf_model/',tf_idf[corpus], num_features=len(dictionary))\n",
    "sims.save(fname=\"text_to_context.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2_docs = [r\"I've not been feeling well lately. I think it was my family, they always get annoying and demanding. Ugh.\"]\n",
    "\n",
    "for line in file2_docs:\n",
    "    query_doc = preprocess_sentence(line)\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    # perform a similarity query against the corpus\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "    # print(document_number, document_similarity)\n",
    "    pred = sims[query_doc_tf_idf]\n",
    "    print('Comparing Result:', pred)\n",
    "    index = topics[np.argmax(pred)]\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = gensim.similarities.Similarity.load('text_to_context.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2_docs = [r\"Lol that's funny.I hate it.\"]\n",
    "\n",
    "for line in file2_docs:\n",
    "    query_doc = preprocess_sentence(line)\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    # perform a similarity query against the corpus\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "    # print(document_number, document_similarity)\n",
    "    pred = loaded_model[query_doc_tf_idf]\n",
    "    print('Comparing Result:', pred)\n",
    "    index = topics[np.argmax(pred)]\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextToContext import TextToContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttc = TextToContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('anxiety', 0.035926), ('depression', 0.021193981), ('self-esteem', 0.02088036), ('relationship-dissolution', 0.016286997), ('marriage', 0.014989097), ('relationships', 0.014099595), ('intimacy', 0.013279834), ('addiction', 0.011866848), ('stress', 0.011655326), ('parenting', 0.010639798), ('self-harm', 0.010303146), ('trauma', 0.00989696), ('counseling-fundamentals', 0.009223869), ('family-conflict', 0.006978291), ('anger-management', 0.0056708725), ('behavioral-change', 0.0043245116), ('domestic-violence', 0.0041926396), ('spirituality', 0.0034696066)]\n"
     ]
    }
   ],
   "source": [
    "ttc.predict(\"I'm trying to learn blender and unity and Lua and it's really stressful when you can't figure something out. Any suggestion how to overcome it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f183e4cb8d58a5b9f43adb2e81d7c0991cbb1caf723ea99f3788b3874e8c5b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
